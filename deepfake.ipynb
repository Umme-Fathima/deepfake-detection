{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Resize and normalize\n",
    "def preprocess_image(image, target_size=(224, 224)):\n",
    "    image = cv2.resize(image, target_size)\n",
    "    image = image / 255.0  # Normalize pixel values to [0, 1]\n",
    "    return image\n",
    "\n",
    "# Augmentation setup\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Resize and normalize\n",
    "def preprocess_image(image, target_size=(224, 224)):\n",
    "    image = cv2.resize(image, target_size)\n",
    "    image = image / 255.0  # Normalize pixel values to [0, 1]\n",
    "    return image\n",
    "\n",
    "# Augmentation setup\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_discriminator(input_shape=(224, 224, 3)):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3, 3), strides=2, padding='same', input_shape=input_shape))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (3, 3), strides=2, padding='same'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(256, (3, 3), strides=2, padding='same'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))  # Output real/fake\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ummef\\deepfake_detection\\deepfake_env\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\ummef\\deepfake_detection\\deepfake_env\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "discriminator = build_discriminator()\n",
    "discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2549 images belonging to 2 classes.\n",
      "Found 568 images belonging to 2 classes.\n",
      "Epoch 1/20\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6564 - loss: 0.6390"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 2s/step - accuracy: 0.6567 - loss: 0.6386 - val_accuracy: 0.8511 - val_loss: 0.4124\n",
      "Epoch 2/20\n",
      "\u001b[1m 1/79\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 1s/step - accuracy: 0.8125 - loss: 0.4499"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ummef\\deepfake_detection\\deepfake_env\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - accuracy: 0.8125 - loss: 0.4499 - val_accuracy: 0.8125 - val_loss: 0.4665\n",
      "Epoch 3/20\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7515 - loss: 0.5212"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 1s/step - accuracy: 0.7514 - loss: 0.5212 - val_accuracy: 0.8033 - val_loss: 0.4002\n",
      "Epoch 4/20\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 0.7188 - loss: 0.4527 - val_accuracy: 0.7996 - val_loss: 0.4447\n",
      "Epoch 5/20\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7657 - loss: 0.4840"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 1s/step - accuracy: 0.7658 - loss: 0.4839 - val_accuracy: 0.8511 - val_loss: 0.3881\n",
      "Epoch 6/20\n",
      "\u001b[1m 1/79\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 1s/step - accuracy: 0.7812 - loss: 0.4730"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.7812 - loss: 0.4730 - val_accuracy: 0.8456 - val_loss: 0.3874\n",
      "Epoch 7/20\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 1s/step - accuracy: 0.7881 - loss: 0.4573 - val_accuracy: 0.8254 - val_loss: 0.4365\n",
      "Epoch 8/20\n",
      "\u001b[1m 1/79\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 1s/step - accuracy: 0.7500 - loss: 0.5228"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - accuracy: 0.7500 - loss: 0.5228 - val_accuracy: 0.8585 - val_loss: 0.3700\n",
      "Epoch 9/20\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 1s/step - accuracy: 0.7776 - loss: 0.4734 - val_accuracy: 0.8309 - val_loss: 0.3759\n",
      "Epoch 10/20\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.8438 - loss: 0.3671 - val_accuracy: 0.8309 - val_loss: 0.4002\n",
      "Epoch 11/20\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7663 - loss: 0.4800"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 1s/step - accuracy: 0.7663 - loss: 0.4801 - val_accuracy: 0.8364 - val_loss: 0.3596\n",
      "Epoch 12/20\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - accuracy: 0.7812 - loss: 0.4922 - val_accuracy: 0.8254 - val_loss: 0.3892\n",
      "Epoch 13/20\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 1s/step - accuracy: 0.7977 - loss: 0.4441 - val_accuracy: 0.8382 - val_loss: 0.3747\n",
      "Epoch 14/20\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.8438 - loss: 0.3941 - val_accuracy: 0.8438 - val_loss: 0.3638\n",
      "Epoch 15/20\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 1s/step - accuracy: 0.8110 - loss: 0.4041 - val_accuracy: 0.8603 - val_loss: 0.3603\n",
      "Epoch 16/20\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.8125 - loss: 0.3601 - val_accuracy: 0.8640 - val_loss: 0.3598\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define training and validation directories\n",
    "train_dir = r\"C:\\Users\\ummef\\Downloads\\dataset\\train\"\n",
    "valid_dir = r\"C:\\Users\\ummef\\Downloads\\dataset\\validation\"\n",
    "\n",
    "# Use ImageDataGenerator for loading and augmentation\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, \n",
    "                                   rotation_range=30, \n",
    "                                   width_shift_range=0.2, \n",
    "                                   height_shift_range=0.2, \n",
    "                                   shear_range=0.2, \n",
    "                                   zoom_range=0.2, \n",
    "                                   horizontal_flip=True)\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(224, 224), batch_size=32, class_mode='binary')\n",
    "valid_generator = valid_datagen.flow_from_directory(valid_dir, target_size=(224, 224), batch_size=32, class_mode='binary')\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_discriminator.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "history = discriminator.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // 32,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=valid_generator.samples // 32,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stop, checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 433 images belonging to 2 classes.\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 390ms/step - accuracy: 0.7651 - loss: 0.5472\n",
      "Test Loss: 0.5157\n",
      "Test Accuracy: 0.7644\n"
     ]
    }
   ],
   "source": [
    "test_dir = r\"C:\\Users\\ummef\\Downloads\\dataset\\test\"\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, target_size=(224, 224), batch_size=32, class_mode='binary')\n",
    "\n",
    "test_loss, test_acc = discriminator.evaluate(test_generator, steps=test_generator.samples // 32)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video is fake.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Function to preprocess the image\n",
    "def preprocess_image(image, target_size=(224, 224)):\n",
    "    img = cv2.resize(image, target_size)\n",
    "    img = img / 255.0\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model('best_discriminator.h5')\n",
    "\n",
    "# Open video stream\n",
    "video_path = r\"C:\\Users\\ummef\\OneDrive\\Documents\\rashmika.mp4\"  # Replace with the path to your video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Variables to track predictions\n",
    "fake_count = 0\n",
    "real_count = 0\n",
    "frame_count = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess the frame\n",
    "    image = preprocess_image(frame)\n",
    "\n",
    "    # Make prediction with verbose=0 to suppress output\n",
    "    prediction = model.predict(image, verbose=0)\n",
    "\n",
    "    # Count predictions\n",
    "    if prediction[0] > 0.7:\n",
    "        fake_count += 1\n",
    "        label = \"Fake\"\n",
    "        color = (0, 0, 255)  # Red color for Fake\n",
    "    else:\n",
    "        real_count += 1\n",
    "        label = \"Real\"\n",
    "        color = (0, 255, 0)  # Green color for Real\n",
    "\n",
    "    # Add the prediction label to the frame\n",
    "    cv2.putText(frame, f\"Prediction: {label}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "    \n",
    "    # Display the frame with prediction label\n",
    "    cv2.imshow('Deepfake Detection', frame)\n",
    "\n",
    "    # Press 'q' to quit the video display\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "# Release the video capture object and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Final prediction based on majority voting\n",
    "if fake_count > real_count:\n",
    "    print(\"The video is fake.\")\n",
    "else:\n",
    "    print(\"The video is real.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video is fake.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to preprocess the image\n",
    "def preprocess_image(image, target_size=(224, 224)):\n",
    "    img = cv2.resize(image, target_size)\n",
    "    img = img / 255.0\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model('best_discriminator.h5')\n",
    "\n",
    "# Open video stream\n",
    "video_path = r\"C:\\Users\\ummef\\OneDrive\\Documents\\rashmika.mp4\"  # Replace with the path to your video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Variables to track predictions and confidence\n",
    "fake_confidences = []\n",
    "real_confidences = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess the frame\n",
    "    image = preprocess_image(frame)\n",
    "\n",
    "    # Make prediction with verbose=0 to suppress output\n",
    "    prediction = model.predict(image, verbose=0)\n",
    "\n",
    "    # Extract the scalar confidence value\n",
    "    fake_confidence = prediction[0][0]\n",
    "    real_confidence = 1 - fake_confidence\n",
    "\n",
    "    # Store confidence values\n",
    "    fake_confidences.append(fake_confidence)\n",
    "    real_confidences.append(real_confidence)\n",
    "\n",
    "# Release the video capture object\n",
    "cap.release()\n",
    "\n",
    "# Calculate overall confidence level\n",
    "mean_fake_confidence = np.mean(fake_confidences)\n",
    "mean_real_confidence = np.mean(real_confidences)\n",
    "\n",
    "# Plot the confidence bar chart\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(['Fake', 'Real'], [mean_fake_confidence, mean_real_confidence], color=['#ff6f61', '#61ff6f'])\n",
    "plt.title('Overall Confidence Levels: Fake vs Real')\n",
    "plt.ylabel('Confidence Level (%)')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "# Final prediction based on majority voting\n",
    "if mean_fake_confidence > mean_real_confidence:\n",
    "    print(\"The video is fake.\")\n",
    "else:\n",
    "    print(\"The video is real.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepfake_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
